{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Embeddings pipeline\n","Here, we use MolFormer, a BERT-trained encoder model to encode our SMILES (without using AIS) into a latent space for similarity pairing. This outputs into a pairs.json file, with the following structure:\n","[[high_scoring_smile, low_scoring_smile, L2_similarity], ...]\n","\n","We also save various checkpoint embeddings (notably, top and bottom json files) for the top 3% and bottom 15%, since these tend to take a while to embed all."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T04:58:59.176331Z","iopub.status.busy":"2024-04-17T04:58:59.176013Z","iopub.status.idle":"2024-04-17T04:59:09.934712Z","shell.execute_reply":"2024-04-17T04:59:09.933715Z","shell.execute_reply.started":"2024-04-17T04:58:59.176304Z"},"trusted":true},"outputs":[],"source":["import torch\n","from transformers import AutoModel, AutoTokenizer\n","\n","# Load in MolFormer for mapping to latent space\n","model = AutoModel.from_pretrained(\"ibm/MoLFormer-XL-both-10pct\", deterministic_eval=True, trust_remote_code=True).to(\"cuda\")\n","tokenizer = AutoTokenizer.from_pretrained(\"ibm/MoLFormer-XL-both-10pct\", trust_remote_code=True)"]},{"cell_type":"markdown","metadata":{},"source":["After loading the model and tokenizer, we read any json files in the working directory (which are fragmented due to earlier saving behavior from docking), and read in SMILES with their associated docking score. The top 3% and bottom 15% get saved, along with their embedding after they are tokenized with MoLFormer's tokenizer and passed through the model. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T04:59:09.937427Z","iopub.status.busy":"2024-04-17T04:59:09.936954Z","iopub.status.idle":"2024-04-17T05:40:29.096593Z","shell.execute_reply":"2024-04-17T05:40:29.095487Z","shell.execute_reply.started":"2024-04-17T04:59:09.937398Z"},"trusted":true},"outputs":[],"source":["import json\n","import torch\n","import os\n","\n","working_directory = '/kaggle/input/total-dataset'\n","combined_scores = {}\n","\n","# Go over each json fragment made from docking\n","for filename in os.listdir(working_directory):\n","    # Check if the file is a JSON file\n","    if filename.endswith('.json'):\n","        full_path = os.path.join(working_directory, filename)\n","        \n","        # Load the JSON data from the file\n","        with open(full_path) as f:\n","            data = json.load(f)\n","        combined_scores.update(data)\n","\n","# Convert the dictionary into a list of tuples [(key, score), ...] and sort by score\n","sorted_items = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n","\n","# Calculate the index ranges for top 3% and bottom 15%\n","num_keys = len(sorted_items)\n","\n","top_3_percent_index = int(num_keys * 0.97)  # Starting index for top 3%\n","bottom_15_percent_index = int(num_keys * 0.15)  # Ending index (exclusive) for bottom 15%\n","\n","# Extract the keys for top 3% and bottom 15%\n","top_3_percent_keys = [item[0] for item in sorted_items[top_3_percent_index:]]\n","bottom_15_percent_keys = [item[0] for item in sorted_items[:bottom_15_percent_index]]\n","        \n","combined_output = {}\n","# Assuming top_3_percent_keys is a list of keys you want to process\n","batch_size = 500\n","top_embeddings = []  # This will store all the embeddings\n","\n","# Process in batches of 500\n","for i in range(0, len(top_3_percent_keys), batch_size):\n","    batch_keys = top_3_percent_keys[i:i + batch_size]\n","\n","    # Tokenize the current batch\n","    encoded_batch = tokenizer(batch_keys, padding='max_length', return_tensors='pt', truncation=True).to(\"cuda\")\n","    \n","    with torch.no_grad():  # Don't compute gradients to save memory and computation\n","        model_output = model(**encoded_batch)\n","\n","        # Assuming the model has a pooler_output attribute (like BERT)\n","        # Convert to float16 to save memory (if necessary)\n","        batch_embeddings = model_output.pooler_output.to(torch.float16).squeeze().tolist()\n","    print(\"Finished top\", i)\n","    # Extend the embeddings list with the embeddings from the current batch\n","    top_embeddings.extend(batch_embeddings)\n","    \n","# Assuming embeddings is a list of embeddings corresponding to each text\n","for i, key in enumerate(top_3_percent_keys):\n","    combined_output[key] = {'embedding': top_embeddings[i], 'score': combined_scores[key]}\n","# Once all files have been processed, write the combined results to a single JSON file\n","with open('combined_output_top.json', 'w') as f_out:\n","    json.dump(combined_output, f_out)\n","\n","bottom_embeddings = []  # This will store all the embeddings\n","\n","# Process in batches of 500\n","for i in range(0, len(bottom_15_percent_keys), batch_size):\n","    batch_keys = bottom_15_percent_keys[i:i + batch_size]\n","\n","    # Tokenize the current batch\n","    encoded_batch = tokenizer(batch_keys, padding='max_length', return_tensors='pt', truncation=True).to(\"cuda\")\n","    \n","    with torch.no_grad():  # Don't compute gradients to save memory and computation\n","        model_output = model(**encoded_batch)\n","\n","        # Assuming the model has a pooler_output attribute (like BERT)\n","        # Convert to float16 to save memory (if necessary)\n","        batch_embeddings = model_output.pooler_output.to(torch.float16).squeeze().tolist()\n","    \n","    print(\"Finished bottom\", i)\n","    # Extend the embeddings list with the embeddings from the current batch\n","    bottom_embeddings.extend(batch_embeddings)\n","\n","combined_output = {}\n","# Assuming embeddings is a list of embeddings corresponding to each text\n","for i, key in enumerate(bottom_15_percent_keys):\n","    combined_output[key] = {'embedding': bottom_embeddings[i], 'score': combined_scores[key]}\n","    \n","# Once all files have been processed, write the combined results to a single JSON file\n","with open('combined_output_bottom.json', 'w') as f_out:\n","    json.dump(combined_output, f_out)"]},{"cell_type":"markdown","metadata":{},"source":["Lastly, pair up the top 3% and bottom 15%, where we exhaust the 3% based on the most similar embedding (in terms of L2) in the bottom 15%. Since we remove both from the respective sets, these pairings are unique, and none of the top or bottom vectors are repeated."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T06:30:24.373956Z","iopub.status.busy":"2024-04-06T06:30:24.373655Z","iopub.status.idle":"2024-04-06T06:30:35.441261Z","shell.execute_reply":"2024-04-06T06:30:35.440168Z","shell.execute_reply.started":"2024-04-06T06:30:24.373930Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import json\n","import faiss\n","import random\n","\n","# Load embeddings from file \n","def load_embeddings(file_path, shuffle=False):\n","    with open(file_path) as f:\n","        data = json.load(f)\n","        keys = list(data.keys())\n","        if shuffle:\n","            np.random.shuffle(keys)\n","        embeddings = np.array([data[key]['embedding'] for key in keys]).astype('float32')\n","    return keys, embeddings\n","\n","# Load top and bottom embeddings\n","top_keys, top_embeddings = load_embeddings('combined_output_top.json', shuffle=True)\n","bottom_keys, bottom_embeddings = load_embeddings('combined_output_bottom.json')\n","\n","# Normalize both for optimizations\n","faiss.normalize_L2(top_embeddings)\n","faiss.normalize_L2(bottom_embeddings)\n","\n","# Create an index for the bottom embeddings\n","index = faiss.IndexFlatIP(bottom_embeddings.shape[1])\n","index.add(bottom_embeddings)\n","\n","# Batch by 100 each so that we don't run out of memory\n","batch_size = 100\n","# get the amount of embeddings\n","n_top = top_embeddings.shape[0]\n","n_bottom = bottom_embeddings.shape[0]\n","# keep track of what we've already used to not repeat\n","used_bottom_indices = set()\n","pairs = []\n","\n","print(f\"Starting processing {n_top} top embeddings in batches of {batch_size}...\")\n","\n","for start_idx in range(0, n_top, batch_size):\n","    end_idx = min(start_idx + batch_size, n_top)\n","    # Look for the nearest neighbors of the top embeddings in the bottom embeddings\n","    D, I = index.search(top_embeddings[start_idx:end_idx], n_bottom)\n","    \n","    for i, indices in enumerate(I):\n","        top_key = top_keys[start_idx + i]\n","        found_match = False\n","        \n","        for idx in indices:\n","            # If we've already used this bottom index, skip it\n","            if idx not in used_bottom_indices:\n","                # Otherwise, we've found a unique pair\n","                bottom_key = bottom_keys[idx]\n","                # Get the similarity score\n","                similarity_score = D[i][np.where(indices == idx)[0][0]]\n","                # Add the pair to the list with it's score\n","                pairs.append((top_key, bottom_key, float(similarity_score)))\n","                used_bottom_indices.add(idx)\n","                found_match = True\n","                break\n","        \n","        if not found_match:\n","            print(f\"No available unique bottom key for top key {top_key}. Consider increasing batch size or revising data.\")\n","    \n","    # Print progress\n","    print(f\"Processed {end_idx} / {n_top} top embeddings. Unique pairs found so far: {len(pairs)}\")\n","\n","# Optionally, sort the pairs by similarity score (highest first)\n","pairs.sort(key=lambda x: x[2], reverse=True)\n","\n","# Save the results\n","with open('pairs.json', 'w') as f_out:\n","    json.dump(pairs, f_out, ensure_ascii=False, indent=4)\n","\n","print(\"Processing complete. Results saved.\")"]}],"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4688523,"sourceId":7969840,"sourceType":"datasetVersion"},{"datasetId":4713804,"sourceId":8004132,"sourceType":"datasetVersion"},{"datasetId":4713817,"sourceId":8004149,"sourceType":"datasetVersion"},{"datasetId":4729146,"sourceId":8024823,"sourceType":"datasetVersion"},{"datasetId":4741531,"sourceId":8042160,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
