{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuff2fullencoded.zst: 49643184475 bytes                                       \n"
     ]
    }
   ],
   "source": [
    "!unzstd shuff2fullencoded.zst -o bigais.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDx5ruYYndV0",
    "outputId": "5d6b7589-d547-45b5-ef7f-55f2b3b594bd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "TAHNmWIbBdqe"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Sequence, Union\n",
    "from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n",
    "import collections\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM , AutoConfig\n",
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "import datasets\n",
    "from transformers import DataCollatorForLanguageModeling \n",
    "from huggingface_hub.hf_api import HfFolder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8nFNiNPBdqf",
    "outputId": "d5d86116-2b5c-47ec-b839-abe88b795f02"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "qG2GgX8vBdqf"
   },
   "outputs": [],
   "source": [
    "\"\"\" AISTokenizer for Hugging Face Transformers.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class AISTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, vocab: str, model_max_length: int, **kwargs):\n",
    "        \"\"\"Character tokens for Hugging Face transformers.\n",
    "\n",
    "        Args:\n",
    "            vocab str: Filename of a file containing with desired tokens\n",
    "            on each newline\n",
    "                    \"<|endoftext|>\": 0\n",
    "\n",
    "                an id (starting at 1) will be assigned to each token.\n",
    "\n",
    "            model_max_length (int): Model maximum sequence length.\n",
    "        \"\"\"\n",
    "        self.vocab = []\n",
    "        with open(vocab, 'r') as file:\n",
    "            for line in file:\n",
    "                self.vocab.append(line.strip())\n",
    "        self.model_max_length = model_max_length\n",
    "        \n",
    "        bos_token = AddedToken(\"<|endoftext|>\", lstrip=False, rstrip=False)\n",
    "        eos_token = AddedToken(\"<|endoftext|>\", lstrip=False, rstrip=False)\n",
    "        pad_token = AddedToken(\"<|pad|>\", lstrip=False, rstrip=False)\n",
    "        unk_token = AddedToken(\"<|endoftext|>\", lstrip=False, rstrip=False)\n",
    "        self._vocab_str_to_int = {\n",
    "            \"<|endoftext|>\": 0,\"<|pad|>\":1,\n",
    "            **{ch: i + 2 for i, ch in enumerate(self.vocab)},\n",
    "        }\n",
    "        self._vocab_int_to_str = {v: k + \" \" for k, v in self._vocab_str_to_int.items()}\n",
    "        self.ids_to_tokens = collections.OrderedDict([(ids, tok) for tok, ids in self._vocab_str_to_int.items()])\n",
    "\n",
    "        super().__init__(\n",
    "            unk_token=unk_token,\n",
    "            bos_token=bos_token,\n",
    "            eos_token=eos_token,\n",
    "            pad_token=pad_token,\n",
    "            add_prefix_space=False,\n",
    "            model_max_length=model_max_length,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self._vocab_str_to_int)\n",
    "\n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        return text.split()\n",
    "\n",
    "    def _convert_token_to_id(self, token: str) -> int:\n",
    "        return self._vocab_str_to_int.get(token, self._vocab_str_to_int[self.unk_token])\n",
    "\n",
    "    def _convert_id_to_token(self, index: int) -> str:\n",
    "        return self._vocab_int_to_str[index]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "    def create_token_type_ids_from_sequences(\n",
    "            self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "        ) -> List[int]:\n",
    "        bos_token_id = []\n",
    "        eos_token_id = []\n",
    "\n",
    "        output = [0] * len(bos_token_id + token_ids_0 + eos_token_id)\n",
    "\n",
    "        if token_ids_1 is not None:\n",
    "            output += [1] * len(bos_token_id + token_ids_1 + eos_token_id)\n",
    "\n",
    "        return output\n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        if True:\n",
    "            bos_token_ids = [self.bos_token_id]\n",
    "        else:\n",
    "            bos_token_ids = []\n",
    "\n",
    "        output = bos_token_ids + token_ids_0\n",
    "\n",
    "        if token_ids_1 is None:\n",
    "            return output\n",
    "\n",
    "        return output + bos_token_ids + token_ids_1\n",
    "\n",
    "\n",
    "    def get_special_tokens_mask(\n",
    "        self,\n",
    "        token_ids_0: List[int],\n",
    "        token_ids_1: Optional[List[int]] = None,\n",
    "        already_has_special_tokens: bool = False,\n",
    "    ) -> List[int]:\n",
    "        if already_has_special_tokens:\n",
    "            if token_ids_1 is not None:\n",
    "                raise ValueError(\n",
    "                    \"You should not supply a second sequence if the provided sequence of \"\n",
    "                    \"ids is already formated with special tokens for the model.\"\n",
    "                )\n",
    "            return list(map(lambda x: 1 if x in [self.bos_token_id, self.eos_token_id] else 0, token_ids_0))\n",
    "\n",
    "        if token_ids_1 is not None:\n",
    "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
    "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
    "    def get_vocab(self) -> Dict[str, int]:\n",
    "        return (self._vocab_str_to_int)\n",
    "    def save_vocabulary(self, vocab_path,filename_prefix: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Save the sentencepiece vocabulary (copy original file) and special tokens file to a directory.\n",
    "        Args:\n",
    "            vocab_path (:obj:`str`):\n",
    "                The directory in which to save the vocabulary.\n",
    "        Returns:\n",
    "            :obj:`Tuple(str)`: Paths to the files saved.\n",
    "        \"\"\"\n",
    "        index = 0\n",
    "        if os.path.isdir(vocab_path):\n",
    "            vocab_file = os.path.join(vocab_path, \"vocab_file.txt\")\n",
    "        else:\n",
    "            vocab_file = vocab_path\n",
    "        with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "            for token, token_index in sorted(self._vocab_str_to_int.items(), key=lambda kv: kv[1]):\n",
    "                if index != token_index:\n",
    "                    print(\n",
    "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
    "                        \" Please check that the vocabulary is not corrupted!\".format(vocab_file)\n",
    "                    )\n",
    "                    index = token_index\n",
    "                writer.write(token + \"\\n\")\n",
    "                index += 1\n",
    "        return (vocab_file,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabfile = \"/workspace/vocab.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "VxWPgWW5pEBj",
    "outputId": "0848975e-feac-4065-b7ff-955273425bf9"
   },
   "outputs": [],
   "source": [
    "context_length = 72\n",
    "tokenizer = AISTokenizer(vocabfile,context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "jx66OGzzTTMJ",
    "outputId": "18609dd0-8b3d-4c19-cd25-aa234f4d8e73"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef8ec45218d4728b01177394e89a206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load in the datasets and convert to memory-mapped for efficiency reasons\n",
    "train_dataset = load_dataset('text', data_files='/workspace/bigais.txt',\n",
    "                        split='train') \n",
    "train_dataset = train_dataset.to_iterable_dataset(num_shards=32).shuffle(buffer_size=10_000)\n",
    "\n",
    "test_dataset = load_dataset('text', data_files='/workspace/AIStest.txt',\n",
    "                        split='train')\n",
    "test_dataset = test_dataset.to_iterable_dataset(num_shards=32).shuffle(buffer_size=10_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# README:\n",
    "# Wrapper around the tokenizer that concatenates samples in each dataset mapping batch. So say it's performing a mapping (which we specified\n",
    "# as mapping this function) on a batch of the dataset. It will take every sample in the batch, concatenate them to one long string, then split\n",
    "# that up into chunks of context_length. This effectively packs the samples, most of which are smaller than context_length, into a tighter\n",
    "# representation which makes training more efficient (otherwise there would be hella padding). It also lets the model learn where the end of\n",
    "# sentence will be as we use EOS token between columns. \n",
    "def tokenize(element):      \n",
    "    # Remove token_type_ids and attention_mask columns\n",
    "    input_ids = tokenizer(element[\"text\"])['input_ids']\n",
    "\n",
    "\n",
    "    # Concatenate input_ids up to max_length\n",
    "    concatenated_ids = [token for sublist in input_ids for token in sublist]\n",
    "    # Split concatenated string into batches\n",
    "    batch_list = [concatenated_ids[i:i+context_length] for i in range(0, len(concatenated_ids), context_length)]\n",
    "    # Return processed output\n",
    "    return {'input_ids': batch_list} # Remove token_type_ids and attention_mask columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.map(tokenize,batched=True,remove_columns=[\"text\"])\n",
    "train_dataset = train_dataset.map(tokenize,batched=True,remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our model parameters\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-v0.1\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    intermediate_size = 1600,\n",
    "    hidden_size = 432,\n",
    "    num_attention_heads = 8, \n",
    "    num_hidden_layers = 12,\n",
    "    max_position_embeddings = context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    sliding_window=context_length,\n",
    "    use_cache=False\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 34.9M parameters\n",
      "Total Parameters: 34871472\n"
     ]
    }
   ],
   "source": [
    "# Load the model. You cannot use flash attention 2 or bfloat16 on t4 or p100 unfortunately. \n",
    "model = AutoModelForCausalLM.from_config(config,torch_dtype=torch.bfloat16,attn_implementation=\"flash_attention_2\")\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"Model size: {model_size/1000**2:.1f}M parameters\")\n",
    "\n",
    "params = model.state_dict()\n",
    "total_params = sum(p.numel() for p in params.values())\n",
    "# printing the number of params\n",
    "print(\"Total Parameters:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This makes our attention masks and labels for us for each batch. Note, do NOT set pad_token to eos_token like many guides to, \n",
    "# this will cause the data_collator to set the label for the eos_token to -100 which leads to it being ignored in loss calculation and\n",
    "# giving us a model that only knows how to yap and not to send a sequence.\n",
    "data_collator = DataCollatorForLanguageModeling (tokenizer,mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   0,  288,  505,  823,    7,  270,  271,  431,    3,  822,    8,  270,\n",
       "          270,  270,  687,    8,  394,    3,  287,    4,  138,    3,   15,  511,\n",
       "            4,  474,  244,  348,    8,  270,  270,    8,    4,  271,    7,    0,\n",
       "          511,   15,  109,    3,  505,  245, 1078,    7, 1130, 1130, 1078,    3,\n",
       "          244,  431,    8,  271,  272,  521,  272,  271,    8,    4, 1130, 1130,\n",
       "            7,    4, 1078,    7, 1131, 1147, 1134, 1147, 1082,    7,  348,    7],\n",
       "        [ 270,  270,    7,    0,  511,   15,  103,    3,  474,  687,    7,  270,\n",
       "          272,  521, 1083,    8, 1080,    3,  387,    4, 1130, 1130, 1130, 1078,\n",
       "            8,    7,    4,  751,    7,    8,  270,  270,  270,  818,    7,  271,\n",
       "          495,  271,    8,    0,  286,  794,    3,  474,  138,    3,  467,    4,\n",
       "           15,  511,    4,  103,    3,   15,  511,    4,  474,  244,  240,   90,\n",
       "            3,  387,    4,    3,  387,    4,  387,    0,  286,  247, 1032,    3]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[   0,  288,  505,  823,    7,  270,  271,  431,    3,  822,    8,  270,\n",
       "          270,  270,  687,    8,  394,    3,  287,    4,  138,    3,   15,  511,\n",
       "            4,  474,  244,  348,    8,  270,  270,    8,    4,  271,    7,    0,\n",
       "          511,   15,  109,    3,  505,  245, 1078,    7, 1130, 1130, 1078,    3,\n",
       "          244,  431,    8,  271,  272,  521,  272,  271,    8,    4, 1130, 1130,\n",
       "            7,    4, 1078,    7, 1131, 1147, 1134, 1147, 1082,    7,  348,    7],\n",
       "        [ 270,  270,    7,    0,  511,   15,  103,    3,  474,  687,    7,  270,\n",
       "          272,  521, 1083,    8, 1080,    3,  387,    4, 1130, 1130, 1130, 1078,\n",
       "            8,    7,    4,  751,    7,    8,  270,  270,  270,  818,    7,  271,\n",
       "          495,  271,    8,    0,  286,  794,    3,  474,  138,    3,  467,    4,\n",
       "           15,  511,    4,  103,    3,   15,  511,    4,  474,  244,  240,   90,\n",
       "            3,  387,    4,    3,  387,    4,  387,    0,  286,  247, 1032,    3]])}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check. Note, the datasets are iterable so you have to wrap them in iter if you want to examine them. You cannot index them with [].\n",
    "it = iter(test_dataset)\n",
    "test = data_collator([next(it) for _ in range(2)])\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pack test, train into one dataset\n",
    "AISDataset = DatasetDict({'train': train_dataset, 'test': test_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "max_steps = 120000 \n",
    "args = TrainingArguments(\n",
    "    bf16=True, \n",
    "    output_dir=\"CHANGE_ME\", \n",
    "    max_steps = max_steps,\n",
    "    per_device_train_batch_size=712,\n",
    "    per_device_eval_batch_size=712,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=int((max_steps/20)),\n",
    "    logging_strategy = \"steps\",\n",
    "    logging_steps=int((max_steps/100)),\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=(max_steps/40),\n",
    "    learning_rate=5e-4,\n",
    "    lr_scheduler_type=\"reduce_lr_on_plateau\",\n",
    "    save_steps=int(max_steps/8),\n",
    "    push_to_hub=True,\n",
    "    logging_first_step=True,\n",
    "    dataloader_num_workers=32,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=AISDataset[\"train\"].with_format(\"torch\"),\n",
    "    eval_dataset=AISDataset[\"test\"].with_format(\"torch\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35533' max='120000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 35533/120000 3:02:41 < 7:14:18, 3.24 it/s, Epoch 0.30/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.666700</td>\n",
       "      <td>0.663473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.655300</td>\n",
       "      <td>0.654182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.650700</td>\n",
       "      <td>0.650171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.648300</td>\n",
       "      <td>0.647097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.646500</td>\n",
       "      <td>0.645241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4688523,
     "sourceId": 7969840,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
