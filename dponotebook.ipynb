{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-11T22:46:11.020819Z","iopub.status.busy":"2024-04-11T22:46:11.020465Z","iopub.status.idle":"2024-04-11T22:46:11.027529Z","shell.execute_reply":"2024-04-11T22:46:11.026535Z","shell.execute_reply.started":"2024-04-11T22:46:11.020792Z"},"trusted":true},"outputs":[],"source":["%pip install atomInSmiles\n","%pip install trl\n","\n","import atomInSmiles as AIS\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import os\n","import copy\n","import json\n","import os\n","from pathlib import Path\n","from typing import Dict, List, Optional, Sequence, Union\n","from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n","import collections\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM , AutoConfig\n","from datasets import load_dataset\n","from datasets import DatasetDict\n","import datasets\n","from transformers import DataCollatorForLanguageModeling \n","from huggingface_hub.hf_api import HfFolder\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# DPO finetuning\n","Here, we finetune our SFT model off of pairings using DPO. We create a PyTorch tokenizer on top of AIS, and load in the pairings we created earlier. We attempt DPO multiple times to find optimized hyperparameters that will help in stable molecular generation (namely, measured by how unique our generated molecule is compared to other outputs and also the inputs given), and output our results when we reach desirable thresholds that measure such qualities."]},{"cell_type":"markdown","metadata":{},"source":["## AISTokenizer\n","Here, we utilize the AIS library for tokenizing raw SMILES into AIS tokens, using a given vocab file with delineated vocab tokens."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T22:40:13.797425Z","iopub.status.busy":"2024-04-11T22:40:13.796683Z","iopub.status.idle":"2024-04-11T22:40:13.820832Z","shell.execute_reply":"2024-04-11T22:40:13.819927Z","shell.execute_reply.started":"2024-04-11T22:40:13.797390Z"},"trusted":true},"outputs":[],"source":["\"\"\" AISTokenizer for Hugging Face Transformers.\n","\n","\"\"\"\n","\n","\n","class AISTokenizer(PreTrainedTokenizer):\n","    def __init__(self, vocab: str, model_max_length: int, **kwargs):\n","        \"\"\"Character tokens for Hugging Face transformers.\n","\n","        Args:\n","            vocab str: Filename of a file containing with desired tokens\n","            on each newline\n","                    \"<|endoftext|>\": 0\n","\n","                an id (starting at 1) will be assigned to each token.\n","\n","            model_max_length (int): Model maximum sequence length.\n","        \"\"\"\n","        self.vocab = []\n","        with open(vocab, 'r') as file:\n","            for line in file:\n","                self.vocab.append(line.strip())\n","        self.model_max_length = model_max_length\n","        \n","        bos_token = AddedToken(\"<|endoftext|>\", lstrip=False, rstrip=False)\n","        eos_token = AddedToken(\"<|endoftext|>\", lstrip=False, rstrip=False)\n","        pad_token = AddedToken(\"<|pad|>\", lstrip=False, rstrip=False)\n","        unk_token = AddedToken(\"<|endoftext|>\", lstrip=False, rstrip=False)\n","        self._vocab_str_to_int = {\n","            \"<|endoftext|>\": 0,\"<|pad|>\":1,\n","            **{ch: i + 2 for i, ch in enumerate(self.vocab)},\n","        }\n","        self._vocab_int_to_str = {v: k + \" \" for k, v in self._vocab_str_to_int.items()}\n","        self.ids_to_tokens = collections.OrderedDict([(ids, tok) for tok, ids in self._vocab_str_to_int.items()])\n","\n","        super().__init__(\n","            unk_token=unk_token,\n","            bos_token=bos_token,\n","            eos_token=eos_token,\n","            pad_token=pad_token,\n","            add_prefix_space=False,\n","            model_max_length=model_max_length,\n","            **kwargs,\n","        )\n","\n","\n","    @property\n","    def vocab_size(self) -> int:\n","        return len(self._vocab_str_to_int)\n","\n","    def _tokenize(self, text: str) -> List[str]:\n","        return text.split()\n","\n","    def _convert_token_to_id(self, token: str) -> int:\n","        return self._vocab_str_to_int.get(token, self._vocab_str_to_int[self.unk_token])\n","\n","    def _convert_id_to_token(self, index: int) -> str:\n","        return self._vocab_int_to_str[index]\n","\n","    def convert_tokens_to_string(self, tokens):\n","        return \"\".join(tokens)\n","\n","    def create_token_type_ids_from_sequences(\n","            self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n","        ) -> List[int]:\n","        bos_token_id = []\n","        eos_token_id = []\n","\n","        output = [0] * len(bos_token_id + token_ids_0 + eos_token_id)\n","\n","        if token_ids_1 is not None:\n","            output += [1] * len(bos_token_id + token_ids_1 + eos_token_id)\n","\n","        return output\n","    def build_inputs_with_special_tokens(\n","        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n","    ) -> List[int]:\n","        if True:\n","            bos_token_ids = [self.bos_token_id]\n","        else:\n","            bos_token_ids = []\n","\n","        output = bos_token_ids + token_ids_0\n","\n","        if token_ids_1 is None:\n","            return output\n","\n","        return output + bos_token_ids + token_ids_1\n","\n","\n","    def get_special_tokens_mask(\n","        self,\n","        token_ids_0: List[int],\n","        token_ids_1: Optional[List[int]] = None,\n","        already_has_special_tokens: bool = False,\n","    ) -> List[int]:\n","        if already_has_special_tokens:\n","            if token_ids_1 is not None:\n","                raise ValueError(\n","                    \"You should not supply a second sequence if the provided sequence of \"\n","                    \"ids is already formated with special tokens for the model.\"\n","                )\n","            return list(map(lambda x: 1 if x in [self.bos_token_id, self.eos_token_id] else 0, token_ids_0))\n","\n","        if token_ids_1 is not None:\n","            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n","        return [1] + ([0] * len(token_ids_0)) + [1]\n","    def get_vocab(self) -> Dict[str, int]:\n","        return (self._vocab_str_to_int)\n","    def save_vocabulary(self, vocab_path,filename_prefix: Optional[str] = None):\n","        \"\"\"\n","        Save the sentencepiece vocabulary (copy original file) and special tokens file to a directory.\n","        Args:\n","            vocab_path (:obj:`str`):\n","                The directory in which to save the vocabulary.\n","        Returns:\n","            :obj:`Tuple(str)`: Paths to the files saved.\n","        \"\"\"\n","        index = 0\n","        if os.path.isdir(vocab_path):\n","            vocab_file = os.path.join(vocab_path, \"vocab_file.txt\")\n","        else:\n","            vocab_file = vocab_path\n","        with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n","            for token, token_index in sorted(self._vocab_str_to_int.items(), key=lambda kv: kv[1]):\n","                if index != token_index:\n","                    print(\n","                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n","                        \" Please check that the vocabulary is not corrupted!\".format(vocab_file)\n","                    )\n","                    index = token_index\n","                writer.write(token + \"\\n\")\n","                index += 1\n","        return (vocab_file,)"]},{"cell_type":"markdown","metadata":{},"source":["Load in the tokenizer with our desired vocab.txt."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T22:40:18.460093Z","iopub.status.busy":"2024-04-11T22:40:18.459618Z","iopub.status.idle":"2024-04-11T22:40:18.471383Z","shell.execute_reply":"2024-04-11T22:40:18.470506Z","shell.execute_reply.started":"2024-04-11T22:40:18.460055Z"},"trusted":true},"outputs":[],"source":["# reading and tokenizing the vocab file\n","vocabfile = \"/kaggle/input/dpotest/vocab.txt\" \n","context_length = 72\n","tokenizer = AISTokenizer(vocabfile,context_length)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T22:40:21.597630Z","iopub.status.busy":"2024-04-11T22:40:21.596953Z","iopub.status.idle":"2024-04-11T22:40:21.685356Z","shell.execute_reply":"2024-04-11T22:40:21.684350Z","shell.execute_reply.started":"2024-04-11T22:40:21.597593Z"},"trusted":true},"outputs":[],"source":["import json\n","# Deserialize our data\n","with open(\"/kaggle/input/dpotest/pairs.json\", \"r\") as file:\n","    data = json.load(file)"]},{"cell_type":"markdown","metadata":{},"source":["Setup DPO pairings from our data loaded in from our pairs file."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T22:22:12.743645Z","iopub.status.busy":"2024-04-11T22:22:12.743346Z","iopub.status.idle":"2024-04-11T22:23:35.300895Z","shell.execute_reply":"2024-04-11T22:23:35.299943Z","shell.execute_reply.started":"2024-04-11T22:22:12.743618Z"},"trusted":true},"outputs":[],"source":["from datasets import Dataset\n","from typing import Dict\n","import atomInSmiles as AIS\n","\n","# Function to get it in the triplet form for DPO\n","def return_prompt_and_responses(samples) -> Dict[str, list]:\n","    prompt = [\"\"] * len(samples[\"good_docker\"])\n","    chosen = [AIS.encode(sample) for sample in samples[\"good_docker\"]]\n","    rejected = [AIS.encode(sample) for sample in samples[\"bad_docker\"]]\n","\n","    return {\n","        \"prompt\": prompt,\n","        \"chosen\": chosen,\n","        \"rejected\": rejected,\n","    }\n","# Turn our serialized json object into a dataset that we can map to the proper DPO format\n","samples_dict = {\n","    \"good_docker\": [pair[0] for pair in data],\n","    \"bad_docker\": [pair[1] for pair in data],\n","}\n","\n","# Load the dataset\n","dataset = Dataset.from_dict(samples_dict)\n","\n","# Map the function to the dataset\n","mapped_dataset = dataset.map(return_prompt_and_responses, batched=True, remove_columns=list(samples_dict.keys()))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T22:41:03.756752Z","iopub.status.busy":"2024-04-11T22:41:03.755830Z","iopub.status.idle":"2024-04-11T22:41:03.777628Z","shell.execute_reply":"2024-04-11T22:41:03.776714Z","shell.execute_reply.started":"2024-04-11T22:41:03.756716Z"},"trusted":true},"outputs":[],"source":["# Defining the data collator\n","from trl.trainer.utils import DPODataCollatorWithPadding\n","data_collator = DPODataCollatorWithPadding(\n","                pad_token_id=tokenizer.pad_token_id,\n","                label_pad_token_id=-100,\n","                is_encoder_decoder=False\n","            )\n"]},{"cell_type":"markdown","metadata":{},"source":["Setup utility functions to test whether or not our model is stable enough for generation."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T22:41:08.063634Z","iopub.status.busy":"2024-04-11T22:41:08.063290Z","iopub.status.idle":"2024-04-11T22:41:08.088004Z","shell.execute_reply":"2024-04-11T22:41:08.087064Z","shell.execute_reply.started":"2024-04-11T22:41:08.063608Z"},"trusted":true},"outputs":[],"source":["# Make a set of every molecule in the dataset to compare novelty against\n","allgenerated = set()\n","for line in data:\n","    allgenerated.add(line[0])\n","    allgenerated.add(line[1])\n","\n","def calc_unique(test_model):\n","    samples = []\n","    novelty = []\n","    for _ in range(250):\n","        # Note: model.generate includes the leading/trailing EOS tokens, so we have to remove them ourselves with [1:-1]\n","        decoded = AIS.decode(tokenizer.decode(test_model.generate(max_new_tokens=72,do_sample=True, temperature=0.5)[0][1:-1]))\n","        samples.append(decoded)\n","        if decoded not in allgenerated:\n","            novelty.append(decoded)\n","    # Calculate samples uniqueness and novelty\n","    unique_elements = set(samples)\n","    novel_elements = set(novelty)\n","    uniqueness_percentage = (len(unique_elements) / len(samples)) * 100\n","    novelty_percentage = (len(novel_elements) / len(samples)) * 100\n","    return uniqueness_percentage, novelty_percentage, unique_elements"]},{"cell_type":"markdown","metadata":{},"source":["## DPO Step\n","Here, we are finally able to start running DPO. We load in our baseline models, and then go over sets of hyperparameters to try various different configurations. If the desired metrics are good enough for generation, we save the hyperparameters, metrics and the unique elements into a file."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T22:46:49.517406Z","iopub.status.busy":"2024-04-11T22:46:49.517016Z","iopub.status.idle":"2024-04-11T22:47:20.825793Z","shell.execute_reply":"2024-04-11T22:47:20.824320Z","shell.execute_reply.started":"2024-04-11T22:46:49.517377Z"},"trusted":true},"outputs":[],"source":["from transformers import Trainer, TrainingArguments\n","from trl import DPOTrainer\n","from transformers.utils import logging\n","logging.set_verbosity_error()\n","\n","# hyperparameters to sweep over\n","epochs = [1, 2, 3]\n","betas = [0.1, 0.3, 0.5]\n","lrs = [1e-5, 3e-5, 5e-5, 1e-4]\n","\n","# sweeping over hyperparameters\n","for epoch in epochs:\n","    for beta in betas:\n","        for lr in lrs:\n","            # loading reference model\n","            model = AutoModelForCausalLM.from_pretrained(\n","                \"victornica/AIS_3\", # location of saved SFT model\n","            )\n","            model_ref = AutoModelForCausalLM.from_pretrained(\n","                \"victornica/AIS_3\", # location of saved SFT model\n","            )\n","            # setting arguments\n","            args = TrainingArguments(\n","                output_dir=\"/kaggle/working/\",\n","                per_device_train_batch_size=128,\n","                per_device_eval_batch_size=128,\n","                num_train_epochs=epoch,\n","                weight_decay=0.1,\n","                learning_rate=lr,\n","                lr_scheduler_type=\"linear\",\n","                report_to=\"none\"\n","            )\n","            # initializing trainer\n","            dpo_trainer = DPOTrainer(\n","                model,\n","                model_ref,\n","                args=args,\n","                beta=beta,\n","                train_dataset=mapped_dataset, \n","                tokenizer=tokenizer,\n","                is_encoder_decoder=False,\n","            )\n","            \n","            # training model\n","            dpo_trainer.train()\n","\n","            # looking at how many unique and novel elements were generated and \n","            # what the unique elements were\n","            uniqueness, novelty, unique_elems = calc_unique(model)\n","            # only track them if 98% of generated elements were unique\n","            if uniqueness >= 98:\n","                file = open(f\"{epoch}-{beta}-{lr}-{uniqueness}-{novelty}.txt\", \"w\")\n","                file.write(f\"{unique_elems}\")\n","                file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T03:04:52.999831Z","iopub.status.busy":"2024-04-06T03:04:52.999418Z","iopub.status.idle":"2024-04-06T03:04:55.459036Z","shell.execute_reply":"2024-04-06T03:04:55.457912Z","shell.execute_reply.started":"2024-04-06T03:04:52.999794Z"},"trusted":true},"outputs":[],"source":["# Sanity check to see if training looked ok\n","for _ in range(5):\n","    # Note: model.generate includes the leading/trailing EOS tokens, so we have to remove them ourselves with [1:-1]\n","    print(AIS.decode(tokenizer.decode(model.generate(max_new_tokens=72,do_sample=True)[0][1:-1])))"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4740955,"sourceId":8041361,"sourceType":"datasetVersion"},{"datasetId":4779371,"sourceId":8149952,"sourceType":"datasetVersion"}],"dockerImageVersionId":30683,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
